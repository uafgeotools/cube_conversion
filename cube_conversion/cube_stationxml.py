#!/usr/bin/env python

"""
Make StationXML files from scratch, given an existing directory of miniSEED files and
metadata generated by cube_convert. Adopted from the ObsPy tutorial here:
    https://docs.obspy.org/tutorial/code_snippets/stationxml_file_from_scratch.html

The code assumes that your sensors are all Chaparral Physics products, that your
digitizers are all DiGOS DATA-CUBE³s, and — IMPORTANTLY — that you only have one sensor
connected to each digitizer (i.e., one channel per station).

Importantly, this code *will not work properly* unless you're using ObsPy after commit
107666b:
    https://github.com/obspy/obspy/tree/107666b9659cbfdc98470a47c003772377551264
(This commit introduced a change that allows for re-computation of the overall
sensitivity for non-standard units — like Pa.) Until the 1.5.0 release of ObsPy comes
out, you can simply update ObsPy via:
    pip install git+https://github.com/obspy/obspy.git
This will fetch and install the newest copy from GitHub.
"""

import argparse
import json
import subprocess
import warnings
from pathlib import Path
from urllib.request import urlretrieve

from obspy import Stream, read
from obspy.clients.nrl import NRL
from obspy.core.inventory import (
    Channel,
    Equipment,
    Inventory,
    Network,
    PolesZerosResponseStage,
    Site,
    Station,
)

# --------------------------------------------------------------------------------------
# Advanced configuration options
# --------------------------------------------------------------------------------------
GAIN = 1  # DiGOS DATA-CUBE³ amplifier gain (this should usually be 1!)
BOB_FACTOR = 10  # Breakout box factor for DATA-CUBE³s (this should usually be 10!)
# --------------------------------------------------------------------------------------

# These names must match EXACTLY what is in the NRL (v2). See here:
# https://ds.iris.edu/ds/nrl/
_SENSOR_MANUFACTURER = 'Chaparral'
_DATALOGGER_MANUFACTURER = 'DiGOSOmnirecs'
_DATALOGGER_MODEL = 'DataCube'

# Ignore warning about Pa units
warnings.filterwarnings(
    'ignore',
    category=UserWarning,
    message="ObsPy can not map unit 'PA' to displacement, velocity, or acceleration",
)


# Define callable main function to work with [project.scripts]
def main():

    # Set up command-line interface
    parser = argparse.ArgumentParser(
        description='Generate StationXML files from DATA-CUBE³ miniSEED files and metadata.',
        allow_abbrev=False,
    )
    parser.add_argument(
        'input_dir',
        help='directory containing miniSEED files and coordinate files produced by cube_convert',
    )
    parser.add_argument(
        'station_mapping',
        nargs='+',
        help='one or more mappings of the form STATION_CODE:CUBE_NAME:SENSOR_SERIAL, for example UAF1:AVJ:903V2',
    )
    parser.add_argument(
        'output_filename', help='filename for the output StationXML file (full path)'
    )
    parser.add_argument(
        'nrl_path',
        help='path to local copy of the NRL (Nominal Response Library) directory',
    )
    parser.add_argument(
        '--validate',
        action='store_true',
        help='run the IRIS StationXML validator on the output file',
    )
    input_args = parser.parse_args()

    # Check if input directory is valid
    input_dir = Path(input_args.input_dir)
    if not input_dir.is_dir():
        raise NotADirectoryError(f'Input directory \'{input_dir}\' doesn\'t ' 'exist.')

    # Parse mappings into dictionary with keys being the station codes (TODO: validate!)
    station_mappings = {}
    for mapping in input_args.station_mapping:
        station_code, cube_name, sensor_serial = mapping.split(':')
        station_mappings[station_code] = {
            'cube_name': cube_name,
            'sensor_serial': sensor_serial,
        }

    # Check if NRL path is a directory
    nrl_path = Path(input_args.nrl_path).expanduser().absolute()
    if not nrl_path.is_dir():
        raise NotADirectoryError(f'NRL path {nrl_path} is not a directory!')

    # Find root directory for cube_conversion repo
    root_dir = Path(__file__).parents[1]

    # Load sensor information including sensitivities, model numbers, etc.
    with open(root_dir / 'sensor_sensitivities.json') as f:
        sensor_info = json.load(f)

    # Read in all data into Stream object for processing
    st = Stream()
    for mseed_file in sorted(input_dir.glob('??.*.*.???.????.???.??')):
        st += read(mseed_file)

    # Process SEED network code
    network_codes = set(tr.stats.network for tr in st)
    assert len(network_codes) == 1, 'Multiple network codes found in data!'
    network_code = network_codes.pop()

    # Process SEED station codes
    station_codes = sorted(set(tr.stats.station for tr in st))

    # Check SEED location codes, as these can indicate if multiple sensors are connected
    # to the same digitizer (not supported by this script)
    location_codes = sorted(set(tr.stats.location for tr in st))
    assert len(location_codes) == 1, 'Multiple location codes found in data!'
    location_code = location_codes.pop()

    # Create Inventory with all required components
    inv = Inventory()
    net = Network(code=network_code)
    for station in station_codes:

        # Get info about this station from mappings and sensor info
        serial_number = station_mappings[station]['sensor_serial']
        cube_name = station_mappings[station]['cube_name']
        sensor_model = sensor_info[serial_number]['model']
        print('\n------------------------------')
        print(f'{network_code}.{station}')
        print('------------------------------')
        print(f'Cube name: {cube_name}')
        print(f'Sensor serial number: {serial_number}')
        print(f'Sensor model: {sensor_model}')

        # Get station start and end times, SEED channel code, sample rate...
        st_station = st.select(station=station)
        station_starttime = min(tr.stats.starttime for tr in st_station)
        station_endtime = max(tr.stats.endtime for tr in st_station)
        channel_codes = sorted(set(tr.stats.channel for tr in st_station))
        assert (
            len(channel_codes) == 1
        ), f'Multiple channel codes found in for station {station}!'
        channel_code = channel_codes.pop()
        sample_rates = sorted(set(tr.stats.sampling_rate for tr in st_station))
        assert len(sample_rates) == 1, 'Multiple sample rates found in data!'
        sample_rate = sample_rates.pop()
        # Read in coordinates from JSON file
        coord_json_file = sorted(
            input_dir.glob(
                f'{network_code}.{station}.{location_code}.{channel_code}.json'
            )
        )
        if len(coord_json_file) == 0:
            raise FileNotFoundError(
                f'No coordinate JSON file found for station {station}!'
            )
        elif len(coord_json_file) > 1:
            raise ValueError  # Almost impossible, but just in case
        else:
            coord_json_file = coord_json_file[0]
        with open(coord_json_file) as f:
            latitude, longitude, elevation = json.load(f)
        # Make Station object
        sta = Station(
            code=station,
            latitude=latitude,
            longitude=longitude,
            elevation=elevation,
            site=Site(name=station),  # Bare minumum... could make this more detailed
            start_date=station_starttime,
            end_date=station_endtime,
        )
        # Define sensor Equipment object
        sensor = Equipment(
            type='Infrasound sensor',
            description=f'{_SENSOR_MANUFACTURER} {sensor_model}',  # MDA shows this!
            manufacturer=_SENSOR_MANUFACTURER,
            model=sensor_model,
            serial_number=serial_number,
        )
        # Define digitizer Equipment object
        data_logger = Equipment(
            type='Digitizer',
            manufacturer=_DATALOGGER_MANUFACTURER,
            model=_DATALOGGER_MODEL,
            serial_number=cube_name,
        )
        # Make Channel object
        cha = Channel(
            code=channel_code,
            location_code=location_code,
            latitude=latitude,  # Could be different if multi-channel setup!
            longitude=longitude,  # Could be different if multi-channel setup!
            elevation=elevation,
            depth=0,  # Required, always 0?
            sample_rate=sample_rate,
            start_date=station_starttime,  # Could be different if multi-channel setup!
            end_date=station_endtime,  # Could be different if multi-channel setup!
            sensor=sensor,
            data_logger=data_logger,
        )
        # Access the local NRL to get response information
        nrl = NRL(str(nrl_path))
        lp_corner = list(nrl.sensors[_SENSOR_MANUFACTURER][sensor_model])
        assert len(lp_corner) == 1, 'Multiple low-pass corner options found!'
        lp_corner = lp_corner[0]
        hf_corner = list(nrl.sensors[_SENSOR_MANUFACTURER][sensor_model][lp_corner])
        assert len(hf_corner) == 1, 'Multiple high-pass corner options found!'
        hf_corner = hf_corner[0]
        sensor_keys = [_SENSOR_MANUFACTURER, sensor_model, lp_corner, hf_corner]
        datalogger_keys = [
            _DATALOGGER_MANUFACTURER,
            _DATALOGGER_MODEL,
            f'{GAIN:g}',
            f'{sample_rate:g} Hz',
        ]
        # The contents of the NRL can be explored interactively in a Python prompt, see
        # API documentation of NRL submodule:
        # http://docs.obspy.org/packages/obspy.clients.nrl.html
        # Here we assume that the end point of data logger and sensor are already known.

        # Get the nominal response for this combination of sensor and digitizer
        try:
            response = nrl.get_response(
                sensor_keys=sensor_keys, datalogger_keys=datalogger_keys
            )
        except KeyError as e:
            msg = (
                f'Could not find response in NRL for sensor keys {sensor_keys} and '
                f'datalogger keys {datalogger_keys}.'
                f'\n\n{nrl.sensors[sensor_keys[0]]}'
            )
            raise Exception(msg) from e
        # KEY: Add a response stage which applies the breakout box factor, after first
        # stage (i.e., right after Pa --> V) — this should be a PZ stage, see:
        # https://docs.fdsn.org/projects/stationxml/en/latest/reference.html#response-stage
        bob_stage_sequence_number = 2
        bob_stage = PolesZerosResponseStage(
            description='DiGOS breakout box voltage step-down',
            stage_sequence_number=bob_stage_sequence_number,
            stage_gain=1 / BOB_FACTOR,
            stage_gain_frequency=1,  # [Hz] Anywhere where the response is flat?
            input_units='V',
            output_units='V',
            pz_transfer_function_type='LAPLACE (RADIANS/SECOND)',
            normalization_frequency=0,
            normalization_factor=1,
            poles=[],
            zeros=[],
        )
        response.response_stages.insert(bob_stage_sequence_number - 1, bob_stage)
        for response_stage in response.response_stages[bob_stage_sequence_number:]:
            response_stage.stage_sequence_number += 1  # Increment following stage #s
        # KEY: Update with our specific measured sensor sensitivity
        nominal_sensitivity = response.response_stages[0].stage_gain
        measured_sensitivity = sensor_info[serial_number]['sensitivity']
        nominal_frequency = response.response_stages[0].stage_gain_frequency
        measured_frequency = sensor_info[serial_number]['frequency']
        response.response_stages[0].stage_gain = measured_sensitivity
        response.response_stages[0].stage_gain_frequency = measured_frequency
        print('Updated sensor sensitivity:')
        print(f'\t{nominal_sensitivity} --> {measured_sensitivity} V/Pa')
        print(f'\t{nominal_frequency:.2f} --> {measured_frequency:.2f} Hz')
        # KEY: Recalculate overall sensitivity (at the calibration frequency)
        response.recalculate_overall_sensitivity(frequency=measured_frequency)

        cha.response = response
        sta.channels.append(cha)
        net.stations.append(sta)

    # Set network start and end dates from station info
    net.start_date = min([sta.start_date for sta in net])
    net.end_date = max([sta.end_date for sta in net])

    inv.networks.append(net)

    # Write to StationXML file
    output_filename = Path(
        input_args.output_filename.rstrip('.xml') + '.xml'
    ).absolute()
    inv.write(output_filename, format='stationxml', validate=True)
    print(f'\nWrote StationXML file to {output_filename}\n')

    # Download and run the StationXML validator, if user wants to
    if input_args.validate:
        # Functions for colored printing... helpful for highlighting errors in validator
        # output!
        # fmt: off
        def print_red(string):
            print('\u001b[31m' + string + '\u001b[0m')
        def print_yellow(string):
            print('\u001b[33m' + string + '\u001b[0m')
        # fmt: on
        # JAR file will downloaded here if it doesn't already exist
        jar_file_path = root_dir / 'stationxml-validator-1.7.5.jar'  # Selects version!
        if jar_file_path.is_file():
            print(f'Found {jar_file_path.name}. Running command:')
        else:
            print(f'Downloading {jar_file_path.name}...')
            url_base = 'https://github.com/iris-edu/stationxml-validator/releases/'
            urlretrieve(
                url_base + f'download/{jar_file_path.stem}/{jar_file_path.name}',
                jar_file_path,
            )
            print('...done. Running command:')
        args = ['java', '-jar', jar_file_path, output_filename]
        print('\t' + ' '.join([str(arg) for arg in args]))
        print('------------------------------')
        print('Output from validator')
        print('------------------------------')
        process = subprocess.run(args, capture_output=True, text=True)
        if process.stderr:  # Something went wrong!
            print_red(process.stderr.strip())
        elif process.stdout:  # Normal operation
            for line in process.stdout.strip().split('\n'):
                if ',Error,' in line:
                    print_red(line)
                elif ',Warning,' in line:
                    print_yellow(line)
                else:
                    print(line)
        else:  # This will never happen?
            raise OSError


# Run the main function if this is called as a script
if __name__ == '__main__':
    main()
