#!/usr/bin/env python

"""
Make StationXML files from scratch, given an existing directory of miniSEED files and
metadata generated by cube_convert. Adopted from the ObsPy tutorial here:
    https://docs.obspy.org/tutorial/code_snippets/stationxml_file_from_scratch.html

The code assumes that your sensors are all Chaparral Physics products, that your
digitizers are all DiGOS DATA-CUBE³s, and — IMPORTANTLY — that you only have one sensor
connected to each digitizer (i.e., one channel per station).

Importantly, this code *will not work properly* unless you're using ObsPy after commit
107666b:
    https://github.com/obspy/obspy/tree/107666b9659cbfdc98470a47c003772377551264
(This commit introduced a change that allows for re-computation of the overall
sensitivity for non-standard units — like Pa.) Until the 1.5.0 release of ObsPy comes
out, you can simply update ObsPy via:
    pip install git+https://github.com/obspy/obspy.git
This will fetch and install the newest copy from GitHub.
"""

import argparse
import json
from pathlib import Path

from obspy import Stream, read
from obspy.clients.nrl import NRL
from obspy.core.inventory import (
    Channel,
    Equipment,
    Inventory,
    Network,
    PolesZerosResponseStage,
    Site,
    Station,
)

# --------------------------------------------------------------------------------------
# Advanced configuration options
# --------------------------------------------------------------------------------------
GAIN = 1  # DiGOS DATA-CUBE³ amplifier gain (this should usually be 1!)
BOB_FACTOR = 10  # Breakout box factor for DATA-CUBE³s (this should usually be 10!)
# --------------------------------------------------------------------------------------

# These names must match EXACTLY what is in the NRL. Unforunately, there are differences
# between the NRL accessed through ObsPy (via online) versus a local copy of the NRL.
# These are likely related to NRL v1 versus v2, but it's not clear. I think the v2 ones
# should be preferred, and they can be viewed here: https://ds.iris.edu/ds/nrl/
# These show up first in the tuples below, the second entries are for the older NRL v1
# and we include those just to cover the bases.
_SENSOR_MANUFACTURER = 'Chaparral', 'Chaparral Physics'
_DATALOGGER_MANUFACTURER = 'DiGOSOmnirecs', 'DiGOS/Omnirecs'
_DATALOGGER_MODEL = 'DataCube', 'DATACUBE'


# Define callable main function to work with [project.scripts]
def main():

    # Set up command-line interface
    parser = argparse.ArgumentParser(
        description='Generate StationXML files from DATA-CUBE³ miniSEED files and metadata.',
        allow_abbrev=False,
    )
    parser.add_argument(
        'input_dir',
        help='directory containing miniSEED files and coordinate files produced by cube_convert',
    )
    parser.add_argument(
        'station_mapping',
        nargs='+',
        help='one or more mappings of the form STATION_CODE:CUBE_NAME:SENSOR_SERIAL, for example UAF1:AVJ:903V2',
    )
    parser.add_argument(
        'output_filename', help='filename for the output StationXML file (full path)'
    )
    parser.add_argument(
        '--nrl-path',
        default=None,
        help='path to local copy of the NRL (if not provided, uses online NRL)',
    )
    input_args = parser.parse_args()

    # Check if input directory is valid
    input_dir = Path(input_args.input_dir)
    if not input_dir.is_dir():
        raise NotADirectoryError(f'Input directory \'{input_dir}\' doesn\'t ' 'exist.')

    # Parse mappings into dictionary with keys being the station codes (TODO: validate!)
    station_mappings = {}
    for mapping in input_args.station_mapping:
        station_code, cube_name, sensor_serial = mapping.split(':')
        station_mappings[station_code] = {
            'cube_name': cube_name,
            'sensor_serial': sensor_serial,
        }

    # Check if NRL path is a directory if provided
    nrl_path = input_args.nrl_path
    if nrl_path is not None:
        nrl_path = Path(nrl_path).absolute()
        if not nrl_path.is_dir():
            raise NotADirectoryError(f'NRL path {nrl_path} is not a directory!')

    # Find root directory for cube_conversion repo
    root_dir = Path(__file__).parents[1]

    # Load sensor information including sensitivities, model numbers, etc.
    with open(root_dir / 'sensor_sensitivities.json') as f:
        sensor_info = json.load(f)

    # Read in all data into Stream object for processing
    st = Stream()
    for mseed_file in sorted(input_dir.glob('??.*.*.???.????.???.??')):
        st += read(mseed_file)

    # Process SEED network code
    network_codes = set(tr.stats.network for tr in st)
    assert len(network_codes) == 1, 'Multiple network codes found in data!'
    network_code = network_codes.pop()

    # Process SEED station codes
    station_codes = sorted(set(tr.stats.station for tr in st))

    # Check SEED location codes, as these can indicate if multiple sensors are connected
    # to the same digitizer (not supported by this script)
    location_codes = sorted(set(tr.stats.location for tr in st))
    assert len(location_codes) == 1, 'Multiple location codes found in data!'
    location_code = location_codes.pop()

    # Create Inventory with all required components
    inv = Inventory()
    net = Network(code=network_code)
    for station in station_codes:
        # Get station start and end times, SEED channel code, sample rate...
        st_station = st.select(station=station)
        station_starttime = min(tr.stats.starttime for tr in st_station)
        station_endtime = max(tr.stats.endtime for tr in st_station)
        channel_codes = sorted(set(tr.stats.channel for tr in st_station))
        assert (
            len(channel_codes) == 1
        ), f'Multiple channel codes found in for station {station}!'
        channel_code = channel_codes.pop()
        sample_rates = sorted(set(tr.stats.sampling_rate for tr in st_station))
        assert len(sample_rates) == 1, 'Multiple sample rates found in data!'
        sample_rate = sample_rates.pop()
        # Read in coordinates from JSON file
        coord_json_file = sorted(
            input_dir.glob(
                f'{network_code}.{station}.{location_code}.{channel_code}.json'
            )
        )
        if len(coord_json_file) == 0:
            raise FileNotFoundError(
                f'No coordinate JSON file found for station {station}!'
            )
        elif len(coord_json_file) > 1:
            raise ValueError  # Almost impossible, but just in case
        else:
            coord_json_file = coord_json_file[0]
        with open(coord_json_file) as f:
            latitude, longitude, elevation = json.load(f)
        # Make Station object
        sta = Station(
            code=station,
            latitude=latitude,
            longitude=longitude,
            elevation=elevation,
            site=Site(name=station),  # Bare minumum... could make this more detailed
            start_date=station_starttime,
            end_date=station_endtime,
        )
        # Define sensor Equipment object
        serial_number = station_mappings[station]['sensor_serial']
        sensor_model = sensor_info[serial_number]['model']
        sensor = Equipment(
            type='Infrasound sensor',
            description=f'{_SENSOR_MANUFACTURER[0]} {sensor_model}',  # MDA shows this!
            manufacturer=_SENSOR_MANUFACTURER[0],
            model=sensor_model,
            serial_number=serial_number,
        )
        # Define digitizer Equipment object
        cube_name = station_mappings[station]['cube_name']
        data_logger = Equipment(
            type='Digitizer',
            manufacturer=_DATALOGGER_MANUFACTURER[0],
            model=_DATALOGGER_MODEL[0],
            serial_number=cube_name,
        )
        # Make Channel object
        cha = Channel(
            code=channel_code,
            location_code=location_code,
            latitude=latitude,
            longitude=longitude,
            elevation=elevation,
            depth=0,  # Required, always 0?
            sample_rate=sample_rate,
            start_date=station_starttime,
            end_date=station_endtime,
            sensor=sensor,
            data_logger=data_logger,
        )
        # Access the NRL to get response information. If the user provided a local path
        # to the NRL, use that; otherwise, use the online NRL.
        if nrl_path is not None:
            nrl = NRL(nrl_path)
            option_ind = 0  # Use v2 NRL entries
        else:
            nrl = NRL()
            option_ind = 1  # Use v1 NRL entries
        # The contents of the NRL can be explored interactively in a Python prompt, see
        # API documentation of NRL submodule:
        # http://docs.obspy.org/packages/obspy.clients.nrl.html
        # Here we assume that the end point of data logger and sensor are already known.

        # Get the nominal response for this combination of sensor and digitizer
        response = nrl.get_response(
            sensor_keys=[_SENSOR_MANUFACTURER[option_ind], sensor_model],
            datalogger_keys=[
                _DATALOGGER_MANUFACTURER[option_ind],
                _DATALOGGER_MODEL[option_ind],
                f'{GAIN:g}',
                f'{sample_rate:g}',
            ],
        )
        # KEY: Add a response stage which applies the breakout box factor, after first
        # stage (i.e., right after Pa --> V) — this should be a PZ stage, see:
        # https://docs.fdsn.org/projects/stationxml/en/latest/reference.html#response-stage
        bob_stage_sequence_number = 2
        bob_stage = PolesZerosResponseStage(
            description='DiGOS breakout box voltage step-down',
            stage_sequence_number=bob_stage_sequence_number,
            stage_gain=1 / BOB_FACTOR,
            stage_gain_frequency=1,  # [Hz] Anywhere where the response is flat?
            input_units='V',
            output_units='V',
            pz_transfer_function_type='LAPLACE (RADIANS/SECOND)',
            normalization_frequency=0,
            normalization_factor=1,
            poles=[],
            zeros=[],
        )
        response.response_stages.insert(bob_stage_sequence_number - 1, bob_stage)
        for response_stage in response.response_stages[bob_stage_sequence_number:]:
            response_stage.stage_sequence_number += 1  # Increment following stage #s
        # KEY: Update with our specific measured sensor sensitivity
        nominal_sensitivity = response.response_stages[0].stage_gain
        measured_sensitivity = sensor_info[serial_number]['sensitivity']
        nominal_frequency = response.response_stages[0].stage_gain_frequency
        measured_frequency = sensor_info[serial_number]['frequency']
        response.response_stages[0].stage_gain = measured_sensitivity
        response.response_stages[0].stage_gain_frequency = measured_frequency
        print(f'{station}: {nominal_sensitivity} --> {measured_sensitivity} V/Pa')
        print(f'{station}: {nominal_frequency:.2f} --> {measured_frequency:.2f} Hz')
        # KEY: Recalculate overall sensitivity (at the calibration frequency)
        response.recalculate_overall_sensitivity(frequency=measured_frequency)

        cha.response = response
        sta.channels.append(cha)
        net.stations.append(sta)

    # Set network start and end dates from station info
    net.start_date = min([sta.start_date for sta in net])
    net.end_date = max([sta.end_date for sta in net])

    inv.networks.append(net)

    # Write to StationXML file
    output_filename = Path(
        input_args.output_filename.rstrip('.xml') + '.xml'
    ).absolute()
    inv.write(output_filename, format='stationxml', validate=True)
    print(f'Wrote StationXML file to {output_filename}\n')


# Run the main function if this is called as a script
if __name__ == '__main__':
    main()
